{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd4d6d7f-5305-4305-b4c6-5f33054a34e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3fe4d-829a-4b88-a0dc-f391ee6ecf94",
   "metadata": {},
   "source": [
    "## Step 1 and 2\n",
    "* Collect all data\n",
    "* Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e6da8d-ee7b-4f06-bab7-a47328fe2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = sorted(os.listdir(\"./dataset\"))\n",
    "\n",
    "images_path = []\n",
    "labels = []\n",
    "for folder in dirs:\n",
    "    path = glob(f\"./data/{folder}/*.jpg\")\n",
    "    label = [f\"{folder}\"] * len(path)\n",
    "    \n",
    "    #append\n",
    "    images_path.extend(path)\n",
    "    labels.extend(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a29b0c41-cf38-4654-b346-2465e33fd857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10246, 10246)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_path), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae92365-56e8-45ed-91cf-3c33ee4de0dd",
   "metadata": {},
   "source": [
    "### Step-3\n",
    "* Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "928bd3d4-ae66-4bca-9800-19098f34ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = images_path[10]\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "cv2.namedWindow(\"original\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"original\", img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00da2453-15e0-4f9c-8331-8b8e7eb9a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# face detecion model\n",
    "face_detection_model = cv2.dnn.readNetFromCaffe(\"./models/deploy.prototxt.txt\", \n",
    "                                               \"./models/res10_300x300_ssd_iter_140000_fp16.caffemodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a0a386f-1af0-4ae0-b2f8-a99861928f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detection_dnn(img):\n",
    "    # blob from image (rgb mean substraction image)\n",
    "    image = img.copy()\n",
    "    h, w = image.shape[:2]\n",
    "    # image, scalefactor, size, mean, swapRB\n",
    "    blob = cv2.dnn.blobFromImage(image, 1, (300, 300), (104, 117, 123), swapRB=True)\n",
    "\n",
    "    # get the detection\n",
    "    face_detection_model.setInput(blob)\n",
    "    detections = face_detection_model.forward()\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # confidence score\n",
    "        confidence = detections[0, 0, i, 2] \n",
    "        if confidence > 0.5: \n",
    "            box = detections[0, 0, i, 3:7]*np.array([w, h, w, h])\n",
    "            box = box.astype(int)\n",
    "\n",
    "            pt1 = (box[0], box[1])\n",
    "            pt2 = (box[2], box[3])\n",
    "\n",
    "            roi = image[box[1]:box[3], box[0]:box[2]]\n",
    "            return roi\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edbfe9f9-4fd3-4db3-a910-f386d0b3d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = face_detection_dnn(img)\n",
    "\n",
    "cv2.namedWindow(\"original\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"original\", img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f5d2f62-84d5-4e09-9834-b2767f3ac4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image blob size: (1, 3, 100, 100), (3, 100, 100), (100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(images_path[2])\n",
    "face = face_detection_dnn(img)\n",
    "if face is not None:\n",
    "    # compution blob from image\n",
    "    blob = cv2.dnn.blobFromImage(face, 1, (100, 100), (104, 107, 123), swapRB=True)\n",
    "    print(f\"Image blob size: {blob.shape}, {np.squeeze(blob).shape}, {np.squeeze(blob).T.shape}\")\n",
    "    \n",
    "    blob_squeeze = np.squeeze(blob).T\n",
    "    blob_rotate = cv2.rotate(blob_squeeze, cv2.ROTATE_90_CLOCKWISE)\n",
    "    blob_flip = cv2.flip(blob_rotate, 1)\n",
    "    \n",
    "    # remove negative value and normalization\n",
    "    max_value = blob_flip.max()\n",
    "    min_value = blob_flip.min()\n",
    "    img_norm = (blob_flip - min_value) / (max_value - min_value)\n",
    "\n",
    "\n",
    "cv2.imshow(\"face\", face)\n",
    "cv2.imshow(\"flip\", blob_flip)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99215bf-77bb-4245-af56-08c8ec66bba1",
   "metadata": {},
   "source": [
    "## Step-5 \n",
    "* blob from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36ff5437-05cf-4ed1-bf5a-c9a3ff419530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapreprocessing(img): \n",
    "    # blob from image(rgb mean substraction image)\n",
    "    image = img.copy()\n",
    "    face = face_detection_dnn(image)\n",
    "    if face is not None:\n",
    "        # compution blob from image\n",
    "        blob = cv2.dnn.blobFromImage(face, 1, (100, 100), (104, 107, 123), swapRB=True)\n",
    "        blob_squeeze = np.squeeze(blob).T\n",
    "        blob_rotate = cv2.rotate(blob_squeeze, cv2.ROTATE_90_CLOCKWISE)\n",
    "        blob_flip = cv2.flip(blob_rotate, 1)\n",
    "\n",
    "        # remove negative value and normalization\n",
    "        max_value = blob_flip.max()\n",
    "        min_value = blob_flip.min()\n",
    "        img_norm = (blob_flip - min_value) / (max_value - min_value)\n",
    "        # img_norm = np.maximum(blob_flip, 0) / blob_flip.max()\n",
    "        return img_norm\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac563db9-c48a-4de9-97c3-b9ab18396685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-123.0, 149.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_flip.min(), blob_flip.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a9630b-02a6-45e2-8bc2-09eb9d6dd66a",
   "metadata": {},
   "source": [
    "## Apply to all Image and Append in a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b081509-b214-45ac-b57b-1607af1cf9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing ... : 10246it [04:23, 38.84it/s]\n"
     ]
    }
   ],
   "source": [
    "len(images_path)\n",
    "\n",
    "data_img = []\n",
    "label_img = []\n",
    "i = 0\n",
    "for path, label in tqdm(zip(images_path, labels), desc=\"Preprocessing ... \"):\n",
    "    img = cv2.imread(path)\n",
    "    process_image = datapreprocessing(img)\n",
    "    if process_image is not None: \n",
    "        data_img.append(process_image)\n",
    "        label_img.append(label)\n",
    "        \n",
    "    i += 1\n",
    "    if i % 100 == 0: \n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "269e3a0e-4db1-41a4-9b6d-b54070da5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data_img)\n",
    "y = np.array(label_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6eed4a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Mask', 'Mask_Chin', 'Mask_Mouth_Chin', 'Mask_Nose_Mouth'],\n",
       "      dtype='<U15')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd19b0a5",
   "metadata": {},
   "source": [
    "### Convert \"y\" to one-hot-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4380211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot = OneHotEncoder()\n",
    "y_onehot = onehot.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3d17c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array = y_onehot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a48145d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10201, 100, 100, 3), (10201, 4))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0c8adfc-966b-49af-8480-8eb7be3167ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"./data_preprocessing/data_preprocesiing.npz\", X, y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ef749-07e5-46a5-9c80-9876ebbdde8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
